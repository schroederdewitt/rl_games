import networks
import models
import common.tr_helpers



"""
lr_schedule: 'adaptive', 'exp_decay', 'linear_decay', 'None'
"""


halfcheetah_config_v2 = {
    'network' : models.ModelA2CContinuousLogStd(networks.default_a2c_network_separated_logstd),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 10.0),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.9,
    'learning_rate' : 1e-4,
    'name' : 'robo1',
    'score_to_win' : 2700,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.000,
    'truncate_grads' : True,
    'env_name' : 'RoboSchoolHalfCheetah-v1',
    'ppo' : True,
    'e_clip' : 0.2,
    'clip_value' : True,
    'num_actors' : 16,
    'steps_num' : 256,
    'minibatch_size' : 1024,
    'mini_epochs' : 4,
    'critic_coef' : 1,
    'lr_schedule' : 'adaptive',
    'lr_threshold' : 0.02,
    'normalize_input' : True,
    'seq_len' : 16,
    'max_epochs' : 10000
    }

roboschoolant_config = {
    'network' : models.ModelA2CContinuous(networks.simple_a2c_network_separated),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 10.0),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.9,
    'learning_rate' : 2.5*1e-4,
    'name' : 'robo1',
    'score_to_win' : 1800,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.000,
    'truncate_grads' : True,
    'env_name' : 'RoboschoolAnt-v1',
    'ppo' : True,
    'e_clip' : 0.2,
    'num_actors' : 16,
    'steps_num' : 256,
    'minibatch_size' : 1024,
    'mini_epochs' : 4,
    'critic_coef' : 1,
    'clip_value' : True,
    'lr_schedule' : 'adaptive',
    'lr_threshold' : 0.02,
    'normalize_input' : False,
    'seq_len' : 16
}


roboschoolhumanoid_config = {
    'network' : models.ModelA2CContinuous(networks.default_a2c_network_separated),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 10.0),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.9,
    'learning_rate' : 1e-4,
    'name' : 'robo1',
    'score_to_win' : 5000,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.000,
    'truncate_grads' : True,
    'env_name' : 'RoboschoolHumanoid-v1',
    'ppo' : True,
    'e_clip' : 0.2,
    'num_actors' : 16,
    'steps_num' : 256,
    'minibatch_size' : 1024,
    'mini_epochs' : 10,
    'critic_coef' : 1,
    'clip_value' : True,
    'normalize_input' : False,
    'lr_schedule' : 'adaptive',
    'lr_threshold' : 0.04,
    'seq_len' : 8
}

roboschoolhumanoid_lstm_config = {
    'network' : models.LSTMModelA2CContinuous(networks.default_a2c_lstm_network_separated),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 10.0),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.9,
    'learning_rate' : 1e-4,
    'name' : 'robo1',
    'score_to_win' : 5000,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.000,
    'truncate_grads' : True,
    'env_name' : 'RoboschoolHumanoid-v1',
    'ppo' : True,
    'e_clip' : 0.2,
    'num_actors' : 16,
    'steps_num' : 128,
    'minibatch_size' : 512,
    'mini_epochs' : 4,
    'critic_coef' : 1,
    'clip_value' : True,
    'lr_schedule' : 'None',
    'normalize_input' : True,
    'seq_len' : 8,
}

flagrun_lstm_config = {
    'network' : models.ModelA2CContinuous(networks.default_a2c_network_separated),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 100.0),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.9,
    'learning_rate' : 1e-4,
    'name' : 'robo1',
    'score_to_win' : 5000,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.000,
    'truncate_grads' : True,
    'env_name' : 'RoboschoolHumanoidFlagrun-v1',
    'ppo' : True,
    'e_clip' : 0.2,
    'num_actors' : 16,
    'steps_num' : 512,
    'minibatch_size' : 1024 * 2,
    'mini_epochs' : 4,
    'critic_coef' : 1,
    'clip_value' : True,
    'lr_schedule' : 'None',
    'normalize_input' : True,
    'seq_len' : 16
}

carracing_config = {
    'network' : models.ModelA2CContinuous(networks.simple_a2c_network),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 100.0),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.9,
    'learning_rate' : 2.5*1e-4,
    'name' : 'robo1',
    'score_to_win' : 5000,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.000,
    'truncate_grads' : True,
    'env_name' : 'CarRacing-v0',
    'ppo' : True,
    'e_clip' : 0.2,
    'num_actors' : 16,
    'steps_num' : 1024,
    'minibatch_size' : 2048,
    'mini_epochs' : 8,
    'critic_coef' : 1,
    'clip_value' : True,
    'lr_schedule' : 'None'
}


quadrupped_config = {
    'network' : models.ModelA2CContinuous(networks.simple_a2c_network),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 100.0),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.9,
    'learning_rate' : 1e-4,
    'name' : 'robo2',
    'score_to_win' : 450000,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.000,
    'truncate_grads' : True,
    'env_name' : 'QuadruppedWalk-v1',
    'ppo' : True,
    'e_clip' : 0.2,
    'num_actors' : 16,
    'steps_num' : 256,
    'minibatch_size' : 1024,
    'mini_epochs' : 4,
    'critic_coef' : 1,
    'clip_value' : True,
    'lr_schedule' : 'None',
    'normalize_input' : False,
    'seq_len' : 16
}


quadrupped_lstm_config = {
    'network' : models.LSTMModelA2CContinuous(networks.default_a2c_lstm_network),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 100.0),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.9,
    'learning_rate' : 1e-4,
    'name' : 'robo2',
    'score_to_win' : 450000,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.000,
    'truncate_grads' : True,
    'env_name' : 'QuadruppedWalk-v1',
    'ppo' : True,
    'e_clip' : 0.2,
    'num_actors' : 16,
    'steps_num' : 256,
    'minibatch_size' : 1024,
    'mini_epochs' : 4,
    'critic_coef' : 1,
    'clip_value' : True,
    'lr_schedule' : 'None',
    'normalize_input' : False,
    'seq_len' : 8
}

bipedalwalker_config = {
    'network' : models.ModelA2CContinuous(networks.simple_a2c_network_separated),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 10.0),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.9,
    'learning_rate' : 1e-4,
    'name' : 'robo1',
    'score_to_win' : 300,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.000,
    'truncate_grads' : True,
    'env_name' : 'BipedalWalker-v2',
    'ppo' : True,
    'e_clip' : 0.2,
    'clip_value' : True,
    'num_actors' : 16,
    'steps_num' : 16,
    'minibatch_size' : 64,
    'mini_epochs' : 4,
    'critic_coef' : 1,
    'lr_schedule' : 'None',
    'max_epochs' : 10000,
    'lr_threshold' : 0.01,
    'normalize_input' : True,
    'seq_len' : 16
}

bipedalwalker_lstm_config = {
    'network' : models.LSTMModelA2CContinuous(networks.default_a2c_lstm_network_separated),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 10.0),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.9,
    'learning_rate' : 1e-4,
    'name' : 'robo1',
    'score_to_win' : 320,
    'grad_norm' : 0.5,
    'entropy_coef' : -0.000,
    'truncate_grads' : True,
    'env_name' : 'BipedalWalker-v2',
    'ppo' : True,
    'e_clip' : 0.2,
    'clip_value' : True,
    'num_actors' : 16,
    'steps_num' : 128,
    'minibatch_size' : 512,
    'mini_epochs' : 4,
    'critic_coef' : 1,
    'lr_schedule' : 'adaptive',
    'lr_threshold' : 0.08,
    'normalize_input' : True,
    'seq_len' : 8
}

bipedalwalkerhardcore__config = {
    'network' : models.ModelA2CContinuousLogStd(networks.default_a2c_network_separated_logstd),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 10.0),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.95,
    'learning_rate' : 1e-4,
    'name' : 'robo1',
    'score_to_win' : 320,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.000,
    'truncate_grads' : True,
    'env_name' : 'BipedalwWlkerHardcore-v2',
    'ppo' : True,
    'e_clip' : 0.1,
    'clip_value' : True,
    'num_actors' : 24,
    'steps_num' : 128,
    'minibatch_size' : 1024,
    'mini_epochs' : 10,
    'critic_coef' : 1,
    'lr_schedule' : 'adaptive',
    'lr_threshold' : 0.01,
    'normalize_input' : False,
    'seq_len' : 8
}

pendulum_lstm_config = {
    'network' : models.LSTMModelA2CContinuous(networks.simple_a2c_lstm_network_separated),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 100.0),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.9,
    'learning_rate' : 2.5e-5,
    'name' : 'robo1',
    'score_to_win' : 0,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.00,
    'truncate_grads' : True,
    'env_name' : 'Pendulum-v0',
    'ppo' : True,
    'e_clip' : 0.2,
    'num_actors' : 16,
    'steps_num' : 64,
    'minibatch_size' : 256,
    'mini_epochs' : 4,
    'critic_coef' : 1,
    'clip_value' : True,
    'lr_schedule' : 'None',
    'lr_threshold' : 0.75,
    'normalize_input' : False
}


bipedalwalkerhardcore_config = {
    'network' : models.ModelA2CContinuous(networks.simple_a2c_network_separated),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 10.0),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.9,
    'learning_rate' : 1e-4,
    'name' : 'robo1',
    'score_to_win' : 300,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.000,
    'truncate_grads' : True,
    'env_name' : 'BipedalWalkerHardcore-v2',
    'ppo' : True,
    'e_clip' : 0.2,
    'clip_value' : True,
    'num_actors' : 16,
    'steps_num' : 256,
    'minibatch_size' : 1024,
    'mini_epochs' : 4,
    'critic_coef' : 1,
    'lr_schedule' : 'None',
    'lr_threshold' : 0.008,
    'normalize_input' : True
}

loonar_config = {
    'network' : models.ModelA2CContinuous(networks.default_a2c_network),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 10.0),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.9,
    'learning_rate' : 1e-4,
    'name' : 'robo1',
    'score_to_win' : 5000,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.000,
    'truncate_grads' : True,
    'env_name' : 'LunarLanderContinuous-v2',
    'ppo' : True,
    'e_clip' : 0.2,
    'num_actors' : 8,
    'steps_num' : 128,
    'minibatch_size' : 256,
    'mini_epochs' : 4,
    'critic_coef' : 1,
    'clip_value' : False,
    'lr_schedule' : 'None'
}

mountain_car_cont_config = {
    'network' : models.ModelA2CContinuous(networks.simple_a2c_network),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.9,
    'learning_rate' : 1e-4,
    'name' : 'robo1',
    'score_to_win' : 5000,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.001,
    'truncate_grads' : True,
    'env_name' : 'MountainCarContinuous-v0',
    'ppo' : True,
    'e_clip' : 0.2,
    'num_actors' : 16,
    'steps_num' : 64,
    'minibatch_size' : 256,
    'mini_epochs' : 4,
    'critic_coef' : 1,
    'clip_value' : True,
    'lr_schedule' : 'None'
}

pendulum_config = {
    'network' : models.ModelA2CContinuous(networks.simple_a2c_network),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 100.0),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.9,
    'learning_rate' : 1e-4,
    'name' : 'robo1',
    'score_to_win' : 0,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.00,
    'truncate_grads' : True,
    'env_name' : 'Pendulum-v0',
    'ppo' : True,
    'e_clip' : 0.2,
    'num_actors' : 16,
    'steps_num' : 64,
    'minibatch_size' : 256,
    'mini_epochs' : 4,
    'critic_coef' : 1,
    'clip_value' : True,
    'lr_schedule' : 'None',
    'lr_threshold' : 0.75,
    'normalize_input' : False
}

mountain_car_config = {
    'network' : models.ModelA2C(networks.simple_a2c_network),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.9,
    'learning_rate' : 1e-4,
    'name' : 'robo1',
    'score_to_win' : 5000,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.01,
    'truncate_grads' : True,
    'env_name' : 'MountainCar-v0',
    'ppo' : True,
    'e_clip' : 0.2,
    'num_actors' : 16,
    'steps_num' : 16,
    'minibatch_size' : 64,
    'mini_epochs' : 4,
    'critic_coef' : 1,
    'clip_value' : True,
    'lr_schedule' : 'None'
}

cartpole_config = {
    'network' : models.ModelA2C(networks.simple_a2c_network),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.9,
    'learning_rate' : 1e-4,
    'name' : 'robo1',
    'score_to_win' : 5000,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.01,
    'truncate_grads' : True,
    'env_name' : 'Cartpole-v1',
    'ppo' : True,
    'e_clip' : 0.2,
    'num_actors' : 16,
    'steps_num' : 16,
    'minibatch_size' : 64,
    'mini_epochs' : 4,
    'critic_coef' : 1,
    'clip_value' : True,
    'lr_schedule' : 'None'
}

car_config = {
    'network' : models.ModelA2CContinuous(networks.atari_a2c_network),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.9,
    'learning_rate' : 1e-3,
    'name' : 'robo1',
    'score_to_win' : 5000,
    'episodes_to_log' : 20, 
    'lives_reward' : 5,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.000,
    'truncate_grads' : True,
    'env_name' : 'MountainCarContinuous-v0',
    'ppo' : True,
    'e_clip' : 0.2,
    'num_actors' : 8,
    'steps_num' : 256,
    'minibatch_size' : 1024,
    'mini_epochs' : 4,
    'critic_coef' : 1.0,
    'clip_value' : True,
    'lr_schedule' : 'None'
}

atari_pong_config = {
    'gamma' : 0.99,
    'tau' : 0.9,
    'network' : models.ModelA2C(networks.atari_a2c_network),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(),
    'normalize_advantage' : True,
    'learning_rate' : 1e-4,
    'name' : 'pong',
    'score_to_win' : 20,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.01,
    'truncate_grads' : True,
    'env_name' : 'PongNoFrameSkip-v4',
    'ppo' : True,
    'e_clip' : 0.1,
    'num_actors' : 8,
    'steps_num' : 128,
    'minibatch_size' : 256,
    'mini_epochs' : 3,
    'critic_coef' : 1.0,
    'clip_value' : True,
    'lr_schedule' : 'None',
    'normalize_input' : False,
    'seq_len' : 16
}

atari_pong_config_lstm = {
    'gamma' : 0.99,
    'tau' : 0.9,
    'network' : models.LSTMModelA2C(networks.atari_a2c_network_lstm),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(),
    'normalize_advantage' : True,
    'learning_rate' : 1e-4,
    'name' : 'pong',
    'score_to_win' : 20,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.01,
    'truncate_grads' : True,
    'env_name' : 'PongNoFrameSkip-v4',
    'ppo' : True,
    'e_clip' : 0.1,
    'num_actors' : 8,
    'steps_num' : 128,
    'minibatch_size' : 256,
    'mini_epochs' : 3,
    'critic_coef' : 1.0,
    'clip_value' : True,
    'lr_schedule' : 'None',
    'normalize_input' : False,
    'seq_len' : 8
}

mario_config_lstm = {
    'gamma' : 0.99,
    'tau' : 0.9,
    'network' : models.LSTMModelA2C(networks.atari_a2c_network_lstm),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 100.0),
    'normalize_advantage' : True,
    'learning_rate' : 1e-4,
    'name' : 'pong',
    'score_to_win' : 100500,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.005,
    'truncate_grads' : True,
    'env_name' : 'SuperMarioBrosRandomStage1-v1',
    'ppo' : True,
    'e_clip' : 0.2,
    'num_actors' : 8,
    'steps_num' : 128,
    'minibatch_size' : 256,
    'mini_epochs' : 3,
    'critic_coef' : 1.0,
    'clip_value' : True,
    'lr_schedule' : 'None',
    'normalize_input' : False,
    'seq_len' : 8
}

mario_random_config_lstm = {
    'gamma' : 0.99,
    'tau' : 0.9,
    'network' : models.LSTMModelA2C(networks.atari_a2c_network_lstm),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 100.0),
    'normalize_advantage' : True,
    'learning_rate' : 1e-4,
    'name' : 'pong',
    'score_to_win' : 100500,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.005,
    'truncate_grads' : True,
    'env_name' : 'SuperMarioBrosRandomStage1-v1',
    'ppo' : True,
    'e_clip' : 0.2,
    'num_actors' : 16,
    'steps_num' : 128,
    'minibatch_size' : 1024,
    'mini_epochs' : 3,
    'critic_coef' : 1.0,
    'clip_value' : True,
    'lr_schedule' : 'None',
    'normalize_input' : False,
    'seq_len' : 8
}


flexant_config = {
    'network' : models.ModelA2CContinuous(networks.default_small_a2c_network_separated),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 100.0),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.9,
    'learning_rate' : 3e-4,
    'name' : 'ant',
    'score_to_win' : 20000,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.000,
    'truncate_grads' : True,
    'env_name' : 'FlexAnt',
    'ppo' : True,
    'e_clip' : 0.2,
    'num_actors' : 400,
    'steps_num' : 24,
    'minibatch_size' : 400 * 16,
    'mini_epochs' : 16,
    'critic_coef' : 1,
    'clip_value' : True,
    'lr_schedule' : 'adaptive',
    'lr_threshold' : 0.01,
    'normalize_input' : False,
    'seq_len' : 16
}

flexhumanoid_config = {
    'network' : models.ModelA2CContinuous(networks.default_small_a2c_network_separated),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 10.0),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.9,
    'learning_rate' : 3e-4,
    'name' : 'hum1',
    'score_to_win' : 15000,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.000,
    'truncate_grads' : True,
    'env_name' : 'FlexHumanoid',
    'ppo' : True,
    'e_clip' : 0.2,
    'num_actors' : 1024,
    'steps_num' : 24,
    'minibatch_size' : 1024 * 16,
    'mini_epochs' : 15,
    'critic_coef' : 1,
    'clip_value' : True,
    'lr_schedule' : 'adaptive',
    'lr_threshold' : 0.02,
    'normalize_input' : False,
    'seq_len' : 16
}

flexhumanoid2_config = {
    'network' : models.ModelA2CContinuousLogStd(networks.default_a2c_network_separated_logstd),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 10.0),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.95,
    'learning_rate' : 1e-4,
    'name' : 'hum2',
    'score_to_win' : 15000,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.000,
    'truncate_grads' : True,
    'env_name' : 'FlexHumanoidHard',
    'ppo' : True,
    'e_clip' : 0.2,
    'num_actors' : 1024,
    'steps_num' : 32,
    'minibatch_size' : 1024 * 16,
    'mini_epochs' : 15,
    'critic_coef' : 2,
    'clip_value' : True,
    'lr_schedule' : 'adaptive',
    'lr_threshold' : 0.01,
    'normalize_input' : False,
    'seq_len' : 8
}

flexhumanoidlstm_config = {
    'network' : models.LSTMModelA2CContinuous(networks.default_a2c_lstm_network_separated),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 10.0),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.9,
    'learning_rate' : 1e-4,
    'name' : 'humlstm',
    'score_to_win' : 10000,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.000,
    'truncate_grads' : True,
    'env_name' : 'FlexHumanoidHard',
    'ppo' : True,
    'e_clip' : 0.1,
    'clip_value' : True,
    'num_actors' : 1024,
    'steps_num' : 32,
    'minibatch_size' : 1024 * 8,
    'mini_epochs' : 4,
    'critic_coef' : 1,
    'lr_schedule' : 'None',
    'lr_threshold' : 0.2,
    'normalize_input' : True,
    'seq_len' : 4,
    'max_epochs' : 10000
    }

flexhumanoidhard_config = {
    'network' : models.ModelA2CContinuousLogStd(networks.default_a2c_network_separated_logstd),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 10.0),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.95,
    'learning_rate' : 1e-4,
    'name' : 'humhard',
    'score_to_win' : 15000,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.0,
    'truncate_grads' : True,
    'env_name' : 'FlexHumanoidHard',
    'ppo' : True,
    'e_clip' : 0.2,
    'num_actors' : 1024,
    'steps_num' : 32,
    'minibatch_size' : 1024 * 16,
    'mini_epochs' : 15,
    'critic_coef' : 2,
    'clip_value' : True,
    'lr_schedule' : 'adaptive',
    'lr_threshold' : 0.02,
    'normalize_input' : True,
    'seq_len' : 8
}

flexhumanoidhardlstm_config = {
    'network' : models.LSTMModelA2CContinuous(networks.default_a2c_lstm_network_separated),
    'reward_shaper' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0 / 10.0),
    'normalize_advantage' : True,
    'gamma' : 0.99,
    'tau' : 0.95,
    'learning_rate' : 1e-4,
    'name' : 'hum2',
    'score_to_win' : 15000,
    'grad_norm' : 0.5,
    'entropy_coef' : 0.0,
    'truncate_grads' : True,
    'env_name' : 'FlexHumanoidHard',
    'ppo' : True,
    'e_clip' : 0.2,
    'num_actors' : 1024,
    'steps_num' : 32,
    'minibatch_size' : 1024 * 8,
    'mini_epochs' : 4,
    'critic_coef' : 1,
    'clip_value' : True,
    'lr_schedule' : 'None',
    'lr_threshold' : 0.02,
    'normalize_input' : True,
    'seq_len' : 4
}
